{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFL Big Data Bowl 2026 - Kaggle Submission Guide\n",
    "\n",
    "This notebook shows how to format predictions for Kaggle submission using the **new Evaluation API**.\n",
    "\n",
    "**Key Changes from Traditional Submissions**:\n",
    "- Uses `kaggle_evaluation.nfl_inference_server` API\n",
    "- Models are called iteratively per time step\n",
    "- No 5-minute time limit for model loading (only inference time matters)\n",
    "- Returns only `x, y` columns (no IDs)\n",
    "\n",
    "**Contents**:\n",
    "1. API Overview\n",
    "2. Model Loading Strategy\n",
    "3. Prediction Function Structure\n",
    "4. Test-Time Augmentation (TTA)\n",
    "5. Complete Submission Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:03.762176Z",
     "iopub.status.busy": "2026-01-15T18:20:03.762099Z",
     "iopub.status.idle": "2026-01-15T18:20:04.607909Z",
     "shell.execute_reply": "2026-01-15T18:20:04.607702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars not installed (pip install polars)\n",
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "# Imports for submission\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Polars for API (required by Kaggle)\n",
    "try:\n",
    "    import polars as pl\n",
    "    print('Polars available for Kaggle API')\n",
    "except ImportError:\n",
    "    print('Polars not installed (pip install polars)')\n",
    "\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kaggle API Overview\n",
    "\n",
    "The 2026 competition uses a new inference API:\n",
    "\n",
    "```python\n",
    "def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        test: DataFrame with columns (game_id, play_id, nfl_id, frame_id)\n",
    "              - These are the frames you need to predict positions for\n",
    "        test_input: DataFrame with historical tracking data\n",
    "              - Contains all input features up to current time\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with ONLY columns: (x, y)\n",
    "        - Must match length of test DataFrame\n",
    "        - Rows must be in same order as test DataFrame\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "**Important Notes**:\n",
    "- Models are loaded ONCE at startup (no time limit)\n",
    "- `predict()` is called repeatedly for each batch\n",
    "- Return only `x, y` columns - API handles row matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.628268Z",
     "iopub.status.busy": "2026-01-15T18:20:04.628107Z",
     "iopub.status.idle": "2026-01-15T18:20:04.737558Z",
     "shell.execute_reply": "2026-01-15T18:20:04.737067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Field constants\n",
    "FIELD_LENGTH = 120.0\n",
    "FIELD_WIDTH = 53.3\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration for submission\"\"\"\n",
    "    # Path to pretrained models (on Kaggle)\n",
    "    MODELS_DIR = Path('/kaggle/input/your-model-dataset/')\n",
    "    \n",
    "    # Local path for testing\n",
    "    LOCAL_MODELS_DIR = Path('../models/st_transformer_demo/')\n",
    "    \n",
    "    WINDOW_SIZE = 10\n",
    "    MAX_FUTURE_HORIZON = 94\n",
    "    N_FOLDS = 5  # or 20 for production\n",
    "    \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cfg = Config()\n",
    "print(f'Device: {cfg.DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Loading Strategy\n",
    "\n",
    "Load models ONCE at startup (no time limit), then reuse for all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.738475Z",
     "iopub.status.busy": "2026-01-15T18:20:04.738397Z",
     "iopub.status.idle": "2026-01-15T18:20:04.741507Z",
     "shell.execute_reply": "2026-01-15T18:20:04.741320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class defined\n"
     ]
    }
   ],
   "source": [
    "# Example model class (same as training)\n",
    "class STTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, horizon=94, hidden_dim=128, n_layers=6, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 10, hidden_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=n_heads,\n",
    "            dim_feedforward=hidden_dim*4, dropout=0.1,\n",
    "            batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.pool_ln = nn.LayerNorm(hidden_dim)\n",
    "        self.pool_attn = nn.MultiheadAttention(hidden_dim, n_heads, batch_first=True)\n",
    "        self.pool_query = nn.Parameter(torch.randn(1, 2, hidden_dim))\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim, 256), nn.GELU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, horizon*2)\n",
    "        )\n",
    "        self.horizon = horizon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_proj(x) + self.pos_embed[:, :T, :]\n",
    "        h = self.encoder(x)\n",
    "        q = self.pool_query.expand(B, -1, -1)\n",
    "        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n",
    "        out = self.head(ctx.flatten(1)).view(B, self.horizon, 2)\n",
    "        return torch.cumsum(out, dim=1)\n",
    "\n",
    "print('Model class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.742687Z",
     "iopub.status.busy": "2026-01-15T18:20:04.742624Z",
     "iopub.status.idle": "2026-01-15T18:20:04.744891Z",
     "shell.execute_reply": "2026-01-15T18:20:04.744722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading function defined\n"
     ]
    }
   ],
   "source": [
    "def load_ensemble_models(model_dir, model_class, n_folds=5, input_dim=16):\n",
    "    \"\"\"\n",
    "    Load all fold models and scalers.\n",
    "    Called ONCE at startup - no time limit.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    scalers = []\n",
    "    \n",
    "    model_dir = Path(model_dir)\n",
    "    \n",
    "    for fold in range(1, n_folds + 1):\n",
    "        # Load model\n",
    "        model_path = model_dir / f'model_fold{fold}.pt'\n",
    "        scaler_path = model_dir / f'scaler_fold{fold}.pkl'\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            print(f'Warning: Model fold {fold} not found')\n",
    "            continue\n",
    "        \n",
    "        # Instantiate and load weights\n",
    "        model = model_class(input_dim)\n",
    "        state = torch.load(model_path, map_location='cpu')\n",
    "        model.load_state_dict(state)\n",
    "        model.to(cfg.DEVICE)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "        \n",
    "        # Load scaler\n",
    "        if scaler_path.exists():\n",
    "            with open(scaler_path, 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "            scalers.append(scaler)\n",
    "    \n",
    "    print(f'Loaded {len(models)} models and {len(scalers)} scalers')\n",
    "    return models, scalers\n",
    "\n",
    "# Example loading (uncomment to test)\n",
    "# models, scalers = load_ensemble_models(cfg.LOCAL_MODELS_DIR, STTransformer)\n",
    "print('Model loading function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction Function\n",
    "\n",
    "The main `predict()` function called by Kaggle API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.745999Z",
     "iopub.status.busy": "2026-01-15T18:20:04.745933Z",
     "iopub.status.idle": "2026-01-15T18:20:04.748257Z",
     "shell.execute_reply": "2026-01-15T18:20:04.748083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction functions defined\n"
     ]
    }
   ],
   "source": [
    "def invert_to_original_direction(x_u, y_u, play_dir_right):\n",
    "    \"\"\"Convert unified coordinates back to original direction.\"\"\"\n",
    "    if not play_dir_right:\n",
    "        return float(x_u), float(y_u)\n",
    "    return float(FIELD_LENGTH - x_u), float(FIELD_WIDTH - y_u)\n",
    "\n",
    "def predict_batch(models, scalers, sequences, device):\n",
    "    \"\"\"\n",
    "    Run ensemble prediction on a batch of sequences.\n",
    "    \n",
    "    Args:\n",
    "        models: List of trained models\n",
    "        scalers: List of corresponding scalers\n",
    "        sequences: List of (window_size, n_features) arrays\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        (dx, dy) predictions averaged across models\n",
    "    \"\"\"\n",
    "    all_preds_dx = []\n",
    "    all_preds_dy = []\n",
    "    \n",
    "    for model, scaler in zip(models, scalers):\n",
    "        # Scale inputs\n",
    "        X_scaled = [scaler.transform(s) for s in sequences]\n",
    "        X_tensor = torch.tensor(np.stack(X_scaled).astype(np.float32)).to(device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_tensor).cpu().numpy()\n",
    "        \n",
    "        all_preds_dx.append(preds[:, :, 0])\n",
    "        all_preds_dy.append(preds[:, :, 1])\n",
    "    \n",
    "    # Average across models\n",
    "    ens_dx = np.mean(all_preds_dx, axis=0)\n",
    "    ens_dy = np.mean(all_preds_dy, axis=0)\n",
    "    \n",
    "    return ens_dx, ens_dy\n",
    "\n",
    "print('Prediction functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test-Time Augmentation (TTA)\n",
    "\n",
    "Improve predictions by averaging original and horizontally flipped predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.749102Z",
     "iopub.status.busy": "2026-01-15T18:20:04.749025Z",
     "iopub.status.idle": "2026-01-15T18:20:04.751255Z",
     "shell.execute_reply": "2026-01-15T18:20:04.751082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA functions defined\n"
     ]
    }
   ],
   "source": [
    "def horizontal_flip_sequence(seq, y_idx=1, vy_idx=None, dir_idx=None):\n",
    "    \"\"\"\n",
    "    Apply horizontal flip to a sequence for TTA.\n",
    "    \n",
    "    Args:\n",
    "        seq: (window_size, n_features) array\n",
    "        y_idx: Index of y coordinate\n",
    "        vy_idx: Index of velocity_y (optional)\n",
    "        dir_idx: Index of direction (optional)\n",
    "    \"\"\"\n",
    "    flipped = seq.copy()\n",
    "    \n",
    "    # Flip y coordinate\n",
    "    flipped[:, y_idx] = FIELD_WIDTH - flipped[:, y_idx]\n",
    "    \n",
    "    # Flip velocity_y\n",
    "    if vy_idx is not None:\n",
    "        flipped[:, vy_idx] = -flipped[:, vy_idx]\n",
    "    \n",
    "    # Flip direction\n",
    "    if dir_idx is not None:\n",
    "        flipped[:, dir_idx] = (180.0 - flipped[:, dir_idx]) % 360.0\n",
    "    \n",
    "    return flipped\n",
    "\n",
    "def predict_with_tta(models, scalers, sequences, device):\n",
    "    \"\"\"\n",
    "    Predict with test-time augmentation (horizontal flip).\n",
    "    Averages original and flipped predictions.\n",
    "    \"\"\"\n",
    "    # Original prediction\n",
    "    dx_orig, dy_orig = predict_batch(models, scalers, sequences, device)\n",
    "    \n",
    "    # Flipped prediction\n",
    "    flipped_seqs = [horizontal_flip_sequence(s, y_idx=1) for s in sequences]\n",
    "    dx_flip, dy_flip = predict_batch(models, scalers, flipped_seqs, device)\n",
    "    \n",
    "    # Average (flip dy back)\n",
    "    dx_tta = (dx_orig + dx_flip) / 2\n",
    "    dy_tta = (dy_orig - dy_flip) / 2  # Flip sign back\n",
    "    \n",
    "    return dx_tta, dy_tta\n",
    "\n",
    "print('TTA functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Submission Example\n",
    "\n",
    "Full submission code structure for Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.751866Z",
     "iopub.status.busy": "2026-01-15T18:20:04.751804Z",
     "iopub.status.idle": "2026-01-15T18:20:04.753772Z",
     "shell.execute_reply": "2026-01-15T18:20:04.753601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission template shown above\n",
      "\n",
      "Key points:\n",
      "  1. Load models ONCE at startup\n",
      "  2. Return ONLY x, y columns\n",
      "  3. Match row count of test DataFrame\n",
      "  4. Clip predictions to field boundaries\n"
     ]
    }
   ],
   "source": [
    "# Complete submission template\n",
    "SUBMISSION_CODE = '''\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Field constants\n",
    "FIELD_LENGTH, FIELD_WIDTH = 120.0, 53.3\n",
    "\n",
    "# Global model storage (loaded once)\n",
    "_models_loaded = False\n",
    "_models = None\n",
    "_scalers = None\n",
    "\n",
    "def load_models_once():\n",
    "    \"\"\"Load models on first call (no time limit)\"\"\"\n",
    "    global _models_loaded, _models, _scalers\n",
    "    \n",
    "    if _models_loaded:\n",
    "        return\n",
    "    \n",
    "    print(\"Loading models...\")\n",
    "    model_dir = Path(\"/kaggle/input/your-model-dataset/\")\n",
    "    \n",
    "    _models = []\n",
    "    _scalers = []\n",
    "    \n",
    "    for fold in range(1, 6):  # 5 folds\n",
    "        # Load model\n",
    "        model = YourModelClass(input_dim=16)\n",
    "        state = torch.load(model_dir / f\"model_fold{fold}.pt\", map_location=\"cpu\")\n",
    "        model.load_state_dict(state)\n",
    "        model.cuda().eval()\n",
    "        _models.append(model)\n",
    "        \n",
    "        # Load scaler\n",
    "        with open(model_dir / f\"scaler_fold{fold}.pkl\", \"rb\") as f:\n",
    "            _scalers.append(pickle.load(f))\n",
    "    \n",
    "    _models_loaded = True\n",
    "    print(f\"Loaded {len(_models)} models\")\n",
    "\n",
    "def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main prediction function called by Kaggle API.\n",
    "    \n",
    "    MUST return DataFrame with ONLY columns: x, y\n",
    "    MUST have same number of rows as test DataFrame\n",
    "    \"\"\"\n",
    "    global _models, _scalers\n",
    "    \n",
    "    # Load models on first call\n",
    "    if not _models_loaded:\n",
    "        load_models_once()\n",
    "    \n",
    "    # Convert to pandas\n",
    "    test_pd = test.to_pandas()\n",
    "    test_input_pd = test_input.to_pandas()\n",
    "    \n",
    "    # 1. Prepare sequences from test_input\n",
    "    sequences = prepare_sequences(test_input_pd, test_pd)\n",
    "    \n",
    "    # 2. Run ensemble prediction\n",
    "    all_preds = []\n",
    "    for model, scaler in zip(_models, _scalers):\n",
    "        X_scaled = [scaler.transform(s) for s in sequences]\n",
    "        X_tensor = torch.tensor(np.stack(X_scaled)).cuda()\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_tensor).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "    \n",
    "    ens_preds = np.mean(all_preds, axis=0)\n",
    "    \n",
    "    # 3. Format output (ONLY x, y columns!)\n",
    "    rows = []\n",
    "    for i, (_, row) in enumerate(test_pd.iterrows()):\n",
    "        # Get prediction for this frame\n",
    "        t = min(row[\"frame_offset\"], 93)  # Clip to horizon\n",
    "        x_pred = last_x[i] + ens_preds[i, t, 0]\n",
    "        y_pred = last_y[i] + ens_preds[i, t, 1]\n",
    "        \n",
    "        # Clip to field\n",
    "        x_pred = np.clip(x_pred, 0, FIELD_LENGTH)\n",
    "        y_pred = np.clip(y_pred, 0, FIELD_WIDTH)\n",
    "        \n",
    "        rows.append({\"x\": x_pred, \"y\": y_pred})\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "'''\n",
    "\n",
    "print('Submission template shown above')\n",
    "print('\\nKey points:')\n",
    "print('  1. Load models ONCE at startup')\n",
    "print('  2. Return ONLY x, y columns')\n",
    "print('  3. Match row count of test DataFrame')\n",
    "print('  4. Clip predictions to field boundaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Kaggle Submission Checklist**:\n",
    "\n",
    "1. **Model Loading**:\n",
    "   - Load all models/scalers at startup\n",
    "   - No time limit for loading phase\n",
    "   - Store in global variables for reuse\n",
    "\n",
    "2. **Prediction Function**:\n",
    "   - Accept `pl.DataFrame` inputs\n",
    "   - Return `pd.DataFrame` with only `x, y` columns\n",
    "   - Row count must match input `test` DataFrame\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Use TTA for ~0.005-0.010 improvement\n",
    "   - Ensemble multiple models/seeds\n",
    "   - Clip predictions to field boundaries\n",
    "   - Handle direction inversion if using unified coordinates\n",
    "\n",
    "**Next**: See `08_ensemble_prediction.ipynb` for combining multiple models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
