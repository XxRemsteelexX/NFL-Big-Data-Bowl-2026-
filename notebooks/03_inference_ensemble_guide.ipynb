{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFL Big Data Bowl 2026 - Inference & Ensemble Guide\n",
    "\n",
    "This notebook explains how to use the pretrained models for inference and create the winning ensemble.\n",
    "\n",
    "**Note**: The complete inference code is in the actual Kaggle submission notebook:\n",
    "- `/mnt/raid0/BigData2/ensemble_4model_SIMPLE.ipynb` (full implementation)\n",
    "\n",
    "**Contents:**\n",
    "1. Understanding the ensemble architecture\n",
    "2. Loading pretrained models\n",
    "3. Making predictions with single models\n",
    "4. Applying Test-Time Augmentation (TTA)\n",
    "5. Creating the 4-model ensemble\n",
    "6. Generating Kaggle submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ensemble Architecture Overview\n",
    "\n",
    "Our best submission (**0.541 Public LB**) uses a 4-model ensemble:\n",
    "\n",
    "```python\n",
    "# Ensemble weights (from ensemble_4model_SIMPLE.ipynb)\n",
    "WEIGHTS = {\n",
    "    'st_transformer_6l': 0.2517,    # 6-Layer ST Transformer\n",
    "    'multiscale_cnn':    0.2517,    # Multiscale CNN + 2L Transformer\n",
    "    'position_st':       0.2490,    # Position-Specific ST Models\n",
    "    'gru_seed27':        0.2476     # GRU with Geometric Features\n",
    "}\n",
    "\n",
    "# Total: 1.0 (equal weighting approximately)\n",
    "```\n",
    "\n",
    "### Why This Ensemble Works:\n",
    "\n",
    "1. **Architecture Diversity**:\n",
    "   - Pure Transformer (ST 6L)\n",
    "   - CNN + Transformer hybrid (Multiscale CNN)\n",
    "   - RNN (GRU)\n",
    "   - Position-specialized (Position ST)\n",
    "\n",
    "2. **Feature Diversity**:\n",
    "   - Global features (ST, CNN)\n",
    "   - Position-specific features (Position ST)\n",
    "   - Geometric features (GRU)\n",
    "\n",
    "3. **Training Diversity**:\n",
    "   - Different seeds\n",
    "   - Different augmentations\n",
    "   - Different window sizes\n",
    "\n",
    "4. **Test-Time Augmentation**:\n",
    "   - All models use horizontal flip TTA\n",
    "   - Consistent +0.005-0.010 improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Inference Implementation\n",
    "\n",
    "### Where to Find the Code:\n",
    "\n",
    "**Full Kaggle Submission Notebook**:\n",
    "```\n",
    "/mnt/raid0/BigData2/ensemble_4model_SIMPLE.ipynb\n",
    "```\n",
    "\n",
    "This notebook contains:\n",
    "- ✅ Complete model loading for all 4 models\n",
    "- ✅ Preprocessing and feature engineering\n",
    "- ✅ Test-Time Augmentation implementation\n",
    "- ✅ Ensemble weighting and combination\n",
    "- ✅ Kaggle submission formatting\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "#### Model 1: ST Transformer (6-Layer)\n",
    "```python\n",
    "# From notebook:\n",
    "DATASET_DIR_ST = '/kaggle/input/6layer-seed700-flip-only'\n",
    "\n",
    "# Load 20 folds\n",
    "for fold in range(20):\n",
    "    model = STTransformer(input_dim)\n",
    "    model.load_state_dict(torch.load(f'model_fold{fold}.pt'))\n",
    "    scaler = joblib.load(f'scaler_fold{fold}.pkl')\n",
    "```\n",
    "\n",
    "#### Model 2: Multiscale CNN\n",
    "```python\n",
    "# From notebook:\n",
    "MODELS_DIR_CNN = Path('/kaggle/input/st-multiscale-cnn-w10-20fold')\n",
    "\n",
    "# Architecture includes multi-scale dilated convolutions\n",
    "class MultiScaleCNN:\n",
    "    conv1: kernel=3, dilation=1\n",
    "    conv2: kernel=3, dilation=2\n",
    "    conv3: kernel=3, dilation=3\n",
    "    # → Concatenate → 2-Layer Transformer\n",
    "```\n",
    "\n",
    "#### Model 3: GRU (Seed 27)\n",
    "```python\n",
    "# From notebook:\n",
    "load_dir = Path('/kaggle/input/gru-w9-seed27-20fold')\n",
    "\n",
    "# Load with geometric features\n",
    "route_kmeans = pickle.load(open('route_kmeans.pkl', 'rb'))\n",
    "route_scaler = pickle.load(open('route_scaler.pkl', 'rb'))\n",
    "\n",
    "# 20-fold ensemble\n",
    "for fold in range(20):\n",
    "    model = JointSeqModel(input_dim, horizon=94, hidden_dim=64)\n",
    "```\n",
    "\n",
    "#### Model 4: Position-Specific ST\n",
    "```python\n",
    "# From notebook:\n",
    "positions = {\n",
    "    'wr': ['WR'],\n",
    "    'te': ['TE'],\n",
    "    'ball_carriers': ['QB', 'RB', 'FB']\n",
    "}\n",
    "\n",
    "# Load separate model for each position (5-fold each)\n",
    "for position_name in positions:\n",
    "    for fold in range(1, 6):\n",
    "        model = STTransformer(input_dim)\n",
    "        model.load_state_dict(torch.load(f'{position_name}/fold{fold}/model.pt'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test-Time Augmentation (TTA)\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "```python\n",
    "def horizontal_flip_dataframe(df):\n",
    "    \"\"\"Flip play horizontally for TTA\"\"\"\n",
    "    df = df.copy()\n",
    "    field_width = 53.3\n",
    "    \n",
    "    # Flip y-coordinate\n",
    "    df['y'] = field_width - df['y']\n",
    "    \n",
    "    # Flip y-velocity\n",
    "    for col in ['velocity_y', 'acceleration_y']:\n",
    "        if col in df.columns:\n",
    "            df[col] = -df[col]\n",
    "    \n",
    "    # Flip direction angles\n",
    "    if 'dir' in df.columns:\n",
    "        df['dir'] = (180 - df['dir']) % 360\n",
    "    if 'o' in df.columns:\n",
    "        df['o'] = (180 - df['o']) % 360\n",
    "    \n",
    "    return df\n",
    "\n",
    "def unflip_predictions(predictions):\n",
    "    \"\"\"Reverse flip on predictions\"\"\"\n",
    "    pred_copy = predictions.copy()\n",
    "    pred_copy[:, :, 1] = -pred_copy[:, :, 1]  # Negate dy\n",
    "    return pred_copy\n",
    "\n",
    "# Usage:\n",
    "# 1. Original predictions\n",
    "pred_original = model.predict(test_input)\n",
    "\n",
    "# 2. Flipped predictions\n",
    "test_input_flip = horizontal_flip_dataframe(test_input)\n",
    "pred_flipped = model.predict(test_input_flip)\n",
    "pred_flipped = unflip_predictions(pred_flipped)\n",
    "\n",
    "# 3. Average both\n",
    "pred_final = (pred_original + pred_flipped) / 2.0\n",
    "```\n",
    "\n",
    "**Impact**: Consistent +0.005-0.010 improvement on all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Combination\n",
    "\n",
    "### From `ensemble_4model_SIMPLE.ipynb`:\n",
    "\n",
    "```python\n",
    "def predict(test, test_input):\n",
    "    \"\"\"\n",
    "    Main prediction function for ensemble\n",
    "    \n",
    "    Args:\n",
    "        test: polars DataFrame with output template\n",
    "        test_input: polars DataFrame with input tracking data\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame with columns ['x', 'y']\n",
    "    \"\"\"\n",
    "    print('Model 1: ST Transformer...')\n",
    "    p1 = predict_model1_st(test, test_input)\n",
    "    \n",
    "    print('Model 2: CNN...')\n",
    "    p2 = predict_model2_cnn(test, test_input)\n",
    "    \n",
    "    print('Model 3: GRU...')\n",
    "    p3 = predict_model3_gru(test, test_input)\n",
    "    \n",
    "    print('Model 4: Position...')\n",
    "    p4 = predict_model4_position(test, test_input)\n",
    "    \n",
    "    print('Ensemble 4 models...')\n",
    "    return pd.DataFrame({\n",
    "        'x': (WEIGHTS['st'] * p1['x'].values + \n",
    "              WEIGHTS['cnn'] * p2['x'].values + \n",
    "              WEIGHTS['gru'] * p3['x'].values + \n",
    "              WEIGHTS['position'] * p4['x'].values),\n",
    "        'y': (WEIGHTS['st'] * p1['y'].values + \n",
    "              WEIGHTS['cnn'] * p2['y'].values + \n",
    "              WEIGHTS['gru'] * p3['y'].values + \n",
    "              WEIGHTS['position'] * p4['y'].values)\n",
    "    })\n",
    "```\n",
    "\n",
    "### Each Model's Prediction Includes:\n",
    "1. Load all folds (20 folds for most models)\n",
    "2. Preprocess input data\n",
    "3. Make predictions with original data\n",
    "4. Make predictions with flipped data (TTA)\n",
    "5. Average flipped predictions after unflipping\n",
    "6. Average all fold predictions\n",
    "7. Return final ensemble prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using the Ensemble\n",
    "\n",
    "### For Local Testing:\n",
    "\n",
    "```python\n",
    "# Copy the code from ensemble_4model_SIMPLE.ipynb\n",
    "# Update the paths to your pretrained models\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Load test data\n",
    "test = pl.read_csv('test_output_template.csv')\n",
    "test_input = pl.read_csv('test_input.csv')\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict(test, test_input)\n",
    "\n",
    "# Save submission\n",
    "predictions.to_csv('submission.csv', index=False)\n",
    "```\n",
    "\n",
    "### For Kaggle Submission:\n",
    "\n",
    "```python\n",
    "# Use the inference server (from notebook)\n",
    "import kaggle_evaluation.nfl_inference_server\n",
    "\n",
    "inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        ('/kaggle/input/nfl-big-data-bowl-2026-prediction/',)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Insights from the Ensemble\n",
    "\n",
    "### What Makes It Work:\n",
    "\n",
    "1. **Diversity is Key**:\n",
    "   - Different architectures (Transformer, CNN, RNN)\n",
    "   - Different feature sets (global, position-specific, geometric)\n",
    "   - Different training strategies (seeds, folds, augmentations)\n",
    "\n",
    "2. **Test-Time Augmentation**:\n",
    "   - Applied to ALL models\n",
    "   - Horizontal flip is most effective\n",
    "   - Consistent improvement across architectures\n",
    "\n",
    "3. **Cross-Validation Averaging**:\n",
    "   - 20-fold CV for most models\n",
    "   - Reduces overfitting\n",
    "   - More stable predictions\n",
    "\n",
    "4. **Weighted Combination**:\n",
    "   - Weights based on inverse Public LB scores\n",
    "   - Nearly equal weights (0.2476 - 0.2517)\n",
    "   - Suggests all models contribute equally\n",
    "\n",
    "### Performance Breakdown:\n",
    "\n",
    "| Model | Individual Score | Weight | Contribution |\n",
    "|-------|-----------------|--------|-------------|\n",
    "| ST Transformer 6L | 0.547 | 0.2517 | 25.17% |\n",
    "| Multiscale CNN | 0.548 | 0.2517 | 25.17% |\n",
    "| Position ST | 0.553 | 0.2490 | 24.90% |\n",
    "| GRU Seed27 | 0.557 | 0.2476 | 24.76% |\n",
    "| **Ensemble** | **0.541** | **1.0** | **100%** |\n",
    "\n",
    "**Improvement**: 0.541 vs 0.547 (best single) = -0.006 gain from ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pretrained Model Locations\n",
    "\n",
    "### Kaggle Datasets (for submissions):\n",
    "\n",
    "```python\n",
    "KAGGLE_DATASETS = {\n",
    "    'st_6l': '/kaggle/input/6layer-seed700-flip-only',\n",
    "    'cnn': '/kaggle/input/st-multiscale-cnn-w10-20fold',\n",
    "    'gru': '/kaggle/input/gru-w9-seed27-20fold',\n",
    "    'position': '/kaggle/input/nfl-bdb-2026-position-st-combined'\n",
    "}\n",
    "```\n",
    "\n",
    "### Local Models:\n",
    "\n",
    "```python\n",
    "LOCAL_MODELS = {\n",
    "    'st_6l': '/mnt/raid0/BigData2/kaggle_submission/6l_no_bad_play',\n",
    "    'cnn': '/mnt/raid0/BigData2/models/4L_CNN_Transformer_NO_BAD_PLAY_20fold_FLIP_SPEED',\n",
    "    'gru': '/mnt/raid0/BigData2/models/gru_w9_h64_flip_speed_seed27_20fold',\n",
    "    'position': '/mnt/raid0/BigData2/kaggle_submission/position_st_models'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. How to Adapt for Your Own Use\n",
    "\n",
    "### Step 1: Copy the Notebook\n",
    "```bash\n",
    "cp /mnt/raid0/BigData2/ensemble_4model_SIMPLE.ipynb ./my_ensemble.ipynb\n",
    "```\n",
    "\n",
    "### Step 2: Update Paths\n",
    "Replace Kaggle dataset paths with your local paths:\n",
    "\n",
    "```python\n",
    "# Before (Kaggle)\n",
    "DATASET_DIR_ST = '/kaggle/input/6layer-seed700-flip-only'\n",
    "\n",
    "# After (Local)\n",
    "DATASET_DIR_ST = '../pretrained/6layer_st_transformer_20fold'\n",
    "```\n",
    "\n",
    "### Step 3: Test Individual Models\n",
    "Test each model separately before combining:\n",
    "\n",
    "```python\n",
    "# Test ST model only\n",
    "p1 = predict_model1_st(test, test_input)\n",
    "print(f\"ST predictions shape: {p1.shape}\")\n",
    "\n",
    "# Test GRU model only\n",
    "p3 = predict_model3_gru(test, test_input)\n",
    "print(f\"GRU predictions shape: {p3.shape}\")\n",
    "```\n",
    "\n",
    "### Step 4: Combine Models\n",
    "Start with 2 models, then add more:\n",
    "\n",
    "```python\n",
    "# 2-model ensemble first\n",
    "ensemble_2 = (0.5 * p1['x'], 0.5 * p3['x'])\n",
    "\n",
    "# Then add more\n",
    "ensemble_4 = (\n",
    "    0.25 * p1['x'] + 0.25 * p2['x'] + \n",
    "    0.25 * p3['x'] + 0.25 * p4['x']\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this guide, we covered:\n",
    "\n",
    "1. ✅ Ensemble architecture (4 models, weighted averaging)\n",
    "2. ✅ Complete inference implementation (in `ensemble_4model_SIMPLE.ipynb`)\n",
    "3. ✅ Test-Time Augmentation (horizontal flip)\n",
    "4. ✅ Model combination strategy (weighted averaging)\n",
    "5. ✅ Pretrained model locations\n",
    "6. ✅ How to adapt for your own use\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Ensemble of 4 diverse models achieves **0.541 Public LB** (best)\n",
    "- TTA provides consistent +0.005-0.010 improvement\n",
    "- Nearly equal weights suggest good diversity\n",
    "- Complete code available in `ensemble_4model_SIMPLE.ipynb`\n",
    "\n",
    "**Next Steps**:\n",
    "- Review the full notebook: `/mnt/raid0/BigData2/ensemble_4model_SIMPLE.ipynb`\n",
    "- Adapt for local inference with your pretrained models\n",
    "- Experiment with different model combinations\n",
    "- Try different weighting strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
