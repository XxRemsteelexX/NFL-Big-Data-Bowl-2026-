{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFL Big Data Bowl 2026 - Geometric Neural Network Training\n",
    "\n",
    "**The Breakthrough Architecture**\n",
    "\n",
    "This notebook implements the Geometric Neural Network approach that achieved **0.559 Public LB**.\n",
    "\n",
    "**Key Insight**: Football players follow geometric rules with learned corrections:\n",
    "- Receivers -> Ball landing (geometric)\n",
    "- Defenders -> Mirror receivers (geometric coupling)\n",
    "- Others -> Momentum (physics)\n",
    "- Model learns only CORRECTIONS to geometric baseline\n",
    "\n",
    "**Architecture**:\n",
    "- Proven GRU + Attention (0.59 base)\n",
    "- 154 proven features (unchanged)\n",
    "- +13 geometric features (the breakthrough)\n",
    "- Train on corrections to geometric baseline\n",
    "\n",
    "**Contents**:\n",
    "1. Setup and Configuration\n",
    "2. Geometric Baseline Computation\n",
    "3. 167-Feature Engineering Pipeline\n",
    "4. GNN-lite Neighbor Embeddings\n",
    "5. Model Architecture\n",
    "6. Training with 5-Fold CV\n",
    "7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:03.565277Z",
     "iopub.status.busy": "2026-01-15T18:20:03.564968Z",
     "iopub.status.idle": "2026-01-15T18:20:04.776103Z",
     "shell.execute_reply": "2026-01-15T18:20:04.775871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.789715Z",
     "iopub.status.busy": "2026-01-15T18:20:04.789572Z",
     "iopub.status.idle": "2026-01-15T18:20:04.792879Z",
     "shell.execute_reply": "2026-01-15T18:20:04.792689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Window size: 10\n",
      "K neighbors: 6\n",
      "Route clusters: 7\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Training configuration for Geometric Neural Network\"\"\"\n",
    "    \n",
    "    # Data paths\n",
    "    DATA_DIR = Path('/mnt/raid0/Kaggle Big Data Bowl/data/raw')\n",
    "    OUTPUT_DIR = Path('../models/gnn_geometric')\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Training\n",
    "    SEED = 42\n",
    "    N_FOLDS = 5\n",
    "    BATCH_SIZE = 256\n",
    "    EPOCHS = 200\n",
    "    PATIENCE = 30\n",
    "    LEARNING_RATE = 1e-3\n",
    "    \n",
    "    # Model\n",
    "    WINDOW_SIZE = 10\n",
    "    HIDDEN_DIM = 128\n",
    "    MAX_FUTURE_HORIZON = 94\n",
    "    \n",
    "    # Field dimensions\n",
    "    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n",
    "    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n",
    "    \n",
    "    # GNN parameters\n",
    "    K_NEIGH = 6      # Number of neighbors\n",
    "    RADIUS = 30.0    # Neighbor radius\n",
    "    TAU = 8.0        # Distance weighting\n",
    "    N_ROUTE_CLUSTERS = 7  # Route pattern clusters\n",
    "    \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "cfg = Config()\n",
    "set_seed(cfg.SEED)\n",
    "\n",
    "print(f'Device: {cfg.DEVICE}')\n",
    "print(f'Window size: {cfg.WINDOW_SIZE}')\n",
    "print(f'K neighbors: {cfg.K_NEIGH}')\n",
    "print(f'Route clusters: {cfg.N_ROUTE_CLUSTERS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Geometric Baseline - THE BREAKTHROUGH\n",
    "\n",
    "The key insight: compute where players SHOULD end up based on geometry, then train the model to learn corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.794255Z",
     "iopub.status.busy": "2026-01-15T18:20:04.794188Z",
     "iopub.status.idle": "2026-01-15T18:20:04.798058Z",
     "shell.execute_reply": "2026-01-15T18:20:04.797883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric baseline functions defined\n"
     ]
    }
   ],
   "source": [
    "def compute_geometric_endpoint(df):\n",
    "    \"\"\"\n",
    "    Compute where each player SHOULD end up based on geometry.\n",
    "    This is the deterministic part - no learning needed.\n",
    "    \n",
    "    Rules:\n",
    "    - Targeted Receivers -> Ball landing spot\n",
    "    - Defensive Coverage -> Mirror receiver position\n",
    "    - Others -> Momentum-based projection\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time to play end\n",
    "    if 'num_frames_output' in df.columns:\n",
    "        t_total = df['num_frames_output'] / 10.0\n",
    "    else:\n",
    "        t_total = 3.0\n",
    "    \n",
    "    df['time_to_endpoint'] = t_total\n",
    "    \n",
    "    # Initialize with momentum (default rule)\n",
    "    df['geo_endpoint_x'] = df['x'] + df['velocity_x'] * t_total\n",
    "    df['geo_endpoint_y'] = df['y'] + df['velocity_y'] * t_total\n",
    "    \n",
    "    # Rule 1: Targeted Receivers converge to ball\n",
    "    if 'ball_land_x' in df.columns:\n",
    "        receiver_mask = df['player_role'] == 'Targeted Receiver'\n",
    "        df.loc[receiver_mask, 'geo_endpoint_x'] = df.loc[receiver_mask, 'ball_land_x']\n",
    "        df.loc[receiver_mask, 'geo_endpoint_y'] = df.loc[receiver_mask, 'ball_land_y']\n",
    "        \n",
    "        # Rule 2: Defenders mirror receivers (maintain offset)\n",
    "        defender_mask = df['player_role'] == 'Defensive Coverage'\n",
    "        has_mirror = df.get('mirror_offset_x', 0).notna() & (df.get('mirror_wr_dist', 50) < 15)\n",
    "        coverage_mask = defender_mask & has_mirror\n",
    "        \n",
    "        df.loc[coverage_mask, 'geo_endpoint_x'] = (\n",
    "            df.loc[coverage_mask, 'ball_land_x'] + \n",
    "            df.loc[coverage_mask, 'mirror_offset_x'].fillna(0)\n",
    "        )\n",
    "        df.loc[coverage_mask, 'geo_endpoint_y'] = (\n",
    "            df.loc[coverage_mask, 'ball_land_y'] + \n",
    "            df.loc[coverage_mask, 'mirror_offset_y'].fillna(0)\n",
    "        )\n",
    "    \n",
    "    # Clip to field boundaries\n",
    "    df['geo_endpoint_x'] = df['geo_endpoint_x'].clip(cfg.FIELD_X_MIN, cfg.FIELD_X_MAX)\n",
    "    df['geo_endpoint_y'] = df['geo_endpoint_y'].clip(cfg.FIELD_Y_MIN, cfg.FIELD_Y_MAX)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_geometric_features(df):\n",
    "    \"\"\"Add features that describe the geometric solution\"\"\"\n",
    "    df = compute_geometric_endpoint(df)\n",
    "    \n",
    "    # Vector to geometric endpoint\n",
    "    df['geo_vector_x'] = df['geo_endpoint_x'] - df['x']\n",
    "    df['geo_vector_y'] = df['geo_endpoint_y'] - df['y']\n",
    "    df['geo_distance'] = np.sqrt(df['geo_vector_x']**2 + df['geo_vector_y']**2)\n",
    "    \n",
    "    # Required velocity to reach geometric endpoint\n",
    "    t = df['time_to_endpoint'] + 0.1\n",
    "    df['geo_required_vx'] = df['geo_vector_x'] / t\n",
    "    df['geo_required_vy'] = df['geo_vector_y'] / t\n",
    "    \n",
    "    # Current velocity vs required\n",
    "    df['geo_velocity_error_x'] = df['geo_required_vx'] - df['velocity_x']\n",
    "    df['geo_velocity_error_y'] = df['geo_required_vy'] - df['velocity_y']\n",
    "    df['geo_velocity_error'] = np.sqrt(\n",
    "        df['geo_velocity_error_x']**2 + df['geo_velocity_error_y']**2\n",
    "    )\n",
    "    \n",
    "    # Required acceleration\n",
    "    t_sq = t * t\n",
    "    df['geo_required_ax'] = (2 * df['geo_vector_x'] / t_sq).clip(-10, 10)\n",
    "    df['geo_required_ay'] = (2 * df['geo_vector_y'] / t_sq).clip(-10, 10)\n",
    "    \n",
    "    # Alignment with geometric path\n",
    "    velocity_mag = np.sqrt(df['velocity_x']**2 + df['velocity_y']**2)\n",
    "    geo_unit_x = df['geo_vector_x'] / (df['geo_distance'] + 0.1)\n",
    "    geo_unit_y = df['geo_vector_y'] / (df['geo_distance'] + 0.1)\n",
    "    df['geo_alignment'] = (\n",
    "        df['velocity_x'] * geo_unit_x + df['velocity_y'] * geo_unit_y\n",
    "    ) / (velocity_mag + 0.1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('Geometric baseline functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GNN-lite Neighbor Embeddings\n",
    "\n",
    "Compute spatial relationships between players using distance-weighted aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.799170Z",
     "iopub.status.busy": "2026-01-15T18:20:04.799089Z",
     "iopub.status.idle": "2026-01-15T18:20:04.803693Z",
     "shell.execute_reply": "2026-01-15T18:20:04.803495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN embedding function defined\n"
     ]
    }
   ],
   "source": [
    "def compute_neighbor_embeddings(input_df, k_neigh=6, radius=30.0, tau=8.0):\n",
    "    \"\"\"GNN-lite: Distance-weighted neighbor feature aggregation\"\"\"\n",
    "    print('Computing GNN neighbor embeddings...')\n",
    "    \n",
    "    cols_needed = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y', \n",
    "                   'velocity_x', 'velocity_y', 'player_side']\n",
    "    src = input_df[cols_needed].copy()\n",
    "    \n",
    "    # Get last frame for each player\n",
    "    last = (src.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "               .groupby(['game_id', 'play_id', 'nfl_id'], as_index=False)\n",
    "               .tail(1)\n",
    "               .rename(columns={'frame_id': 'last_frame_id'})\n",
    "               .reset_index(drop=True))\n",
    "    \n",
    "    # Self-join to get all player pairs\n",
    "    tmp = last.merge(\n",
    "        src.rename(columns={\n",
    "            'frame_id': 'nb_frame_id', 'nfl_id': 'nfl_id_nb',\n",
    "            'x': 'x_nb', 'y': 'y_nb', \n",
    "            'velocity_x': 'vx_nb', 'velocity_y': 'vy_nb', \n",
    "            'player_side': 'player_side_nb'\n",
    "        }),\n",
    "        left_on=['game_id', 'play_id', 'last_frame_id'],\n",
    "        right_on=['game_id', 'play_id', 'nb_frame_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Remove self-connections\n",
    "    tmp = tmp[tmp['nfl_id_nb'] != tmp['nfl_id']]\n",
    "    \n",
    "    # Compute relative features\n",
    "    tmp['dx'] = tmp['x_nb'] - tmp['x']\n",
    "    tmp['dy'] = tmp['y_nb'] - tmp['y']\n",
    "    tmp['dvx'] = tmp['vx_nb'] - tmp['velocity_x']\n",
    "    tmp['dvy'] = tmp['vy_nb'] - tmp['velocity_y']\n",
    "    tmp['dist'] = np.sqrt(tmp['dx']**2 + tmp['dy']**2)\n",
    "    \n",
    "    # Filter by radius\n",
    "    tmp = tmp[np.isfinite(tmp['dist']) & (tmp['dist'] > 1e-6)]\n",
    "    if radius is not None:\n",
    "        tmp = tmp[tmp['dist'] <= radius]\n",
    "    \n",
    "    # Ally/opponent flag\n",
    "    tmp['is_ally'] = (tmp['player_side_nb'] == tmp['player_side']).astype(np.float32)\n",
    "    \n",
    "    # Rank neighbors by distance\n",
    "    keys = ['game_id', 'play_id', 'nfl_id']\n",
    "    tmp['rnk'] = tmp.groupby(keys)['dist'].rank(method='first')\n",
    "    if k_neigh is not None:\n",
    "        tmp = tmp[tmp['rnk'] <= float(k_neigh)]\n",
    "    \n",
    "    # Distance weighting\n",
    "    tmp['w'] = np.exp(-tmp['dist'] / float(tau))\n",
    "    sum_w = tmp.groupby(keys)['w'].transform('sum')\n",
    "    tmp['wn'] = np.where(sum_w > 0, tmp['w'] / sum_w, 0.0)\n",
    "    \n",
    "    # Separate ally/opponent weights\n",
    "    tmp['wn_ally'] = tmp['wn'] * tmp['is_ally']\n",
    "    tmp['wn_opp'] = tmp['wn'] * (1.0 - tmp['is_ally'])\n",
    "    \n",
    "    # Weighted relative features\n",
    "    for col in ['dx', 'dy', 'dvx', 'dvy']:\n",
    "        tmp[f'{col}_ally_w'] = tmp[col] * tmp['wn_ally']\n",
    "        tmp[f'{col}_opp_w'] = tmp[col] * tmp['wn_opp']\n",
    "    \n",
    "    tmp['dist_ally'] = np.where(tmp['is_ally'] > 0.5, tmp['dist'], np.nan)\n",
    "    tmp['dist_opp'] = np.where(tmp['is_ally'] < 0.5, tmp['dist'], np.nan)\n",
    "    \n",
    "    # Aggregate\n",
    "    ag = tmp.groupby(keys).agg(\n",
    "        gnn_ally_dx_mean=('dx_ally_w', 'sum'),\n",
    "        gnn_ally_dy_mean=('dy_ally_w', 'sum'),\n",
    "        gnn_ally_dvx_mean=('dvx_ally_w', 'sum'),\n",
    "        gnn_ally_dvy_mean=('dvy_ally_w', 'sum'),\n",
    "        gnn_opp_dx_mean=('dx_opp_w', 'sum'),\n",
    "        gnn_opp_dy_mean=('dy_opp_w', 'sum'),\n",
    "        gnn_opp_dvx_mean=('dvx_opp_w', 'sum'),\n",
    "        gnn_opp_dvy_mean=('dvy_opp_w', 'sum'),\n",
    "        gnn_ally_cnt=('is_ally', 'sum'),\n",
    "        gnn_opp_cnt=('is_ally', lambda s: float(len(s) - s.sum())),\n",
    "        gnn_ally_dmin=('dist_ally', 'min'),\n",
    "        gnn_ally_dmean=('dist_ally', 'mean'),\n",
    "        gnn_opp_dmin=('dist_opp', 'min'),\n",
    "        gnn_opp_dmean=('dist_opp', 'mean'),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Fill missing\n",
    "    for c in ag.columns:\n",
    "        if c not in keys:\n",
    "            ag[c] = ag[c].fillna(0.0 if 'cnt' in c or 'mean' in c else radius)\n",
    "    \n",
    "    return ag\n",
    "\n",
    "print('GNN embedding function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Route Pattern Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.804676Z",
     "iopub.status.busy": "2026-01-15T18:20:04.804607Z",
     "iopub.status.idle": "2026-01-15T18:20:04.808272Z",
     "shell.execute_reply": "2026-01-15T18:20:04.808082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route pattern extraction defined\n"
     ]
    }
   ],
   "source": [
    "def extract_route_patterns(input_df, kmeans=None, scaler=None, fit=True):\n",
    "    \"\"\"Cluster trajectory patterns using KMeans\"\"\"\n",
    "    route_features = []\n",
    "    \n",
    "    for (gid, pid, nid), group in tqdm(input_df.groupby(['game_id', 'play_id', 'nfl_id']), \n",
    "                                        desc='Extracting routes', leave=False):\n",
    "        traj = group.sort_values('frame_id').tail(5)\n",
    "        \n",
    "        if len(traj) < 3:\n",
    "            continue\n",
    "        \n",
    "        positions = traj[['x', 'y']].values\n",
    "        speeds = traj['s'].values\n",
    "        \n",
    "        # Trajectory metrics\n",
    "        total_dist = np.sum(np.sqrt(np.diff(positions[:, 0])**2 + np.diff(positions[:, 1])**2))\n",
    "        displacement = np.sqrt((positions[-1, 0] - positions[0, 0])**2 + \n",
    "                              (positions[-1, 1] - positions[0, 1])**2)\n",
    "        straightness = displacement / (total_dist + 0.1)\n",
    "        \n",
    "        # Turn analysis\n",
    "        angles = np.arctan2(np.diff(positions[:, 1]), np.diff(positions[:, 0]))\n",
    "        if len(angles) > 1:\n",
    "            angle_changes = np.abs(np.diff(angles))\n",
    "            max_turn = np.max(angle_changes)\n",
    "            mean_turn = np.mean(angle_changes)\n",
    "        else:\n",
    "            max_turn = mean_turn = 0\n",
    "        \n",
    "        route_features.append({\n",
    "            'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n",
    "            'traj_straightness': straightness,\n",
    "            'traj_max_turn': max_turn,\n",
    "            'traj_mean_turn': mean_turn,\n",
    "            'traj_depth': abs(positions[-1, 0] - positions[0, 0]),\n",
    "            'traj_width': abs(positions[-1, 1] - positions[0, 1]),\n",
    "            'speed_mean': speeds.mean(),\n",
    "            'speed_change': speeds[-1] - speeds[0] if len(speeds) > 1 else 0,\n",
    "        })\n",
    "    \n",
    "    route_df = pd.DataFrame(route_features)\n",
    "    feat_cols = ['traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n",
    "                 'traj_depth', 'traj_width', 'speed_mean', 'speed_change']\n",
    "    X = route_df[feat_cols].fillna(0)\n",
    "    \n",
    "    if fit:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        kmeans = KMeans(n_clusters=cfg.N_ROUTE_CLUSTERS, random_state=cfg.SEED, n_init=10)\n",
    "        route_df['route_pattern'] = kmeans.fit_predict(X_scaled)\n",
    "        return route_df, kmeans, scaler\n",
    "    else:\n",
    "        X_scaled = scaler.transform(X)\n",
    "        route_df['route_pattern'] = kmeans.predict(X_scaled)\n",
    "        return route_df\n",
    "\n",
    "print('Route pattern extraction defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture - GRU + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.809103Z",
     "iopub.status.busy": "2026-01-15T18:20:04.809033Z",
     "iopub.status.idle": "2026-01-15T18:20:04.972640Z",
     "shell.execute_reply": "2026-01-15T18:20:04.972416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 360,892\n"
     ]
    }
   ],
   "source": [
    "class JointSeqModel(nn.Module):\n",
    "    \"\"\"Proven GRU + Attention architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, horizon):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.pool_ln = nn.LayerNorm(128)\n",
    "        self.pool_attn = nn.MultiheadAttention(128, num_heads=4, batch_first=True)\n",
    "        self.pool_query = nn.Parameter(torch.randn(1, 1, 128))\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128, 256), \n",
    "            nn.GELU(), \n",
    "            nn.Dropout(0.2), \n",
    "            nn.Linear(256, horizon * 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, _ = self.gru(x)\n",
    "        B = h.size(0)\n",
    "        q = self.pool_query.expand(B, -1, -1)\n",
    "        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n",
    "        out = self.head(ctx.squeeze(1))\n",
    "        out = out.view(B, -1, 2)\n",
    "        return torch.cumsum(out, dim=1)\n",
    "\n",
    "# Test model\n",
    "model = JointSeqModel(167, cfg.MAX_FUTURE_HORIZON).to(cfg.DEVICE)\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Function - Temporal Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.973804Z",
     "iopub.status.busy": "2026-01-15T18:20:04.973736Z",
     "iopub.status.idle": "2026-01-15T18:20:04.976141Z",
     "shell.execute_reply": "2026-01-15T18:20:04.975951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function defined\n"
     ]
    }
   ],
   "source": [
    "class TemporalHuber(nn.Module):\n",
    "    \"\"\"Huber loss with time decay weighting\"\"\"\n",
    "    def __init__(self, delta=0.5, time_decay=0.03):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "        self.time_decay = time_decay\n",
    "    \n",
    "    def forward(self, pred, target, mask):\n",
    "        err = pred - target\n",
    "        abs_err = torch.abs(err)\n",
    "        huber = torch.where(abs_err <= self.delta, \n",
    "                           0.5 * err * err, \n",
    "                           self.delta * (abs_err - 0.5 * self.delta))\n",
    "        \n",
    "        if self.time_decay > 0:\n",
    "            L = pred.size(1)\n",
    "            t = torch.arange(L, device=pred.device).float()\n",
    "            weight = torch.exp(-self.time_decay * t).view(1, L, 1)\n",
    "            huber = huber * weight\n",
    "            mask = mask.unsqueeze(-1) * weight\n",
    "        \n",
    "        return (huber * mask).sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n",
    "print('Loss function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Data\n",
    "\n",
    "Load training data (using 2 weeks for demo, use all 18 for production)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:20:04.977235Z",
     "iopub.status.busy": "2026-01-15T18:20:04.977169Z",
     "iopub.status.idle": "2026-01-15T18:20:05.424497Z",
     "shell.execute_reply": "2026-01-15T18:20:05.424217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Week 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Week 2\n",
      "\n",
      "Loaded:\n",
      "  Input:  574,300 rows\n",
      "  Output: 64,268 rows\n"
     ]
    }
   ],
   "source": [
    "# Load data (demo: 2 weeks)\n",
    "weeks_to_load = [1, 2]  # Use range(1, 19) for full training\n",
    "\n",
    "print('Loading training data...')\n",
    "input_dfs, output_dfs = [], []\n",
    "\n",
    "for week in weeks_to_load:\n",
    "    input_file = cfg.DATA_DIR / f'input_2023_w{week:02d}.csv'\n",
    "    output_file = cfg.DATA_DIR / f'output_2023_w{week:02d}.csv'\n",
    "    \n",
    "    if input_file.exists() and output_file.exists():\n",
    "        input_dfs.append(pd.read_csv(input_file))\n",
    "        output_dfs.append(pd.read_csv(output_file))\n",
    "        print(f'  Week {week}')\n",
    "\n",
    "train_input = pd.concat(input_dfs, ignore_index=True)\n",
    "train_output = pd.concat(output_dfs, ignore_index=True)\n",
    "\n",
    "print(f'\\nLoaded:')\n",
    "print(f'  Input:  {len(train_input):,} rows')\n",
    "print(f'  Output: {len(train_output):,} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "This notebook demonstrates the Geometric Neural Network approach:\n",
    "\n",
    "1. **Geometric Baseline**: Compute deterministic endpoints based on player roles\n",
    "2. **GNN Embeddings**: Distance-weighted neighbor feature aggregation\n",
    "3. **Route Patterns**: K-means clustering of trajectory shapes\n",
    "4. **167 Features**: 154 proven + 13 geometric breakthrough features\n",
    "5. **GRU + Attention**: Proven architecture for sequence modeling\n",
    "\n",
    "**Key Insight**: Train the model to learn CORRECTIONS to the geometric baseline, not raw predictions.\n",
    "\n",
    "**Next Steps**:\n",
    "- Run full feature engineering pipeline\n",
    "- Train with 5-fold cross-validation\n",
    "- See `06_gru_training.ipynb` for the full GRU implementation\n",
    "- See `07_kaggle_submission.ipynb` for submission format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
